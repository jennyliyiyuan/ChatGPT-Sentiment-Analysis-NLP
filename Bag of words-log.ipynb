{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGYIsVpj3LwH",
    "outputId": "7266aa96-458c-4877-d2bb-a3b646517266"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "! pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ImJkJy7D3Tjg",
    "outputId": "d075ade3-57c7-4320-8e1c-f2510255e216"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('file.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXntf_38a9gN"
   },
   "outputs": [],
   "source": [
    "df.drop(labels='Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXxP91Uv6PF_"
   },
   "outputs": [],
   "source": [
    "# Remove all the tweet links since they all begin with https:\n",
    "df['tweet_list'] = df['tweets'].str.split('https:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtncAvVR6gAj"
   },
   "outputs": [],
   "source": [
    "# Select the text part of the list\n",
    "\n",
    "tweets = [i[0] for i in df.tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBiYVQbB6_WJ"
   },
   "outputs": [],
   "source": [
    "# Import re for string processing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q86vPXCLXz6x"
   },
   "outputs": [],
   "source": [
    "# Remove all non-alphanumeric characters from the text list\n",
    "\n",
    "string = r'[A-Za-z0-9 ]'\n",
    "\n",
    "trim_list=[]\n",
    "\n",
    "for row in tweets:\n",
    "    s=''\n",
    "    for letter in row:\n",
    "        if bool(re.match(string, letter)):\n",
    "            s+=letter\n",
    "    trim_list.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIuN1YnlX3BC"
   },
   "outputs": [],
   "source": [
    "# Remove the non-printing characters from text\n",
    "\n",
    "rep_list = ['\\U0001fae1', '\\\\n', '@', '#', '\\xa0', '***', 'https', 'nhttps']\n",
    "\n",
    "for i in trim_list:\n",
    "    for j in rep_list:\n",
    "        if j in i:\n",
    "            i.replace(j,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFWDrj9RX_jT",
    "outputId": "5e7b92b0-296f-4b4f-dac2-f195c5b9d83f"
   },
   "outputs": [],
   "source": [
    "df['tweets'] = trim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-pejjqVmYEBg",
    "outputId": "7f9d56fc-45f4-4ae1-e92a-c83001172cbe"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1X4HlFEauax",
    "outputId": "f603d0fd-d321-4b16-e1f6-6fbda9f746bb"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYJYVkz6at73"
   },
   "source": [
    "describe and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "drzxfBP0asiB",
    "outputId": "64e555da-ca1c-4f1d-ca4a-8a9e4fd27f53"
   },
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "mTdauNFqe0-f",
    "outputId": "a2f6a9a6-d385-4426-f39d-813c014c980d"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the frequency of each category in the 'labels' column\n",
    "label_counts = df['labels'].value_counts()\n",
    "\n",
    "# Create a count plot\n",
    "sns.countplot(x='labels', data=df)\n",
    "\n",
    "# Set the labels for the plot\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A67pQOnvoSoG",
    "outputId": "3cd75f52-de58-4906-9fcb-e9474142c783"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset into a pandas dataframe\n",
    "df = pd.read_csv('file.csv')\n",
    "# Check for missing values\n",
    "print('Missing values:\\n', df.isnull().sum())\n",
    "# Print summary statistics\n",
    "print('Summary statistics:\\n', df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "dE_z1D4nr7nR",
    "outputId": "fa1cb6c4-5679-4337-c3d5-8226196593b5"
   },
   "outputs": [],
   "source": [
    "# Create a pie chart of label percentages\n",
    "label_percentages = df['labels'].value_counts(normalize=True)\n",
    "plt.pie(label_percentages.values, labels=label_percentages.index, autopct='%1.1f%%')\n",
    "\n",
    "# Add chart title\n",
    "plt.title('Percentage of Labels')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3pF93Zjo84r",
    "outputId": "bb05aee9-e2ff-4430-e6bb-5cc0f7953046"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "JffZaPfwoSoG",
    "outputId": "6d26c9a1-a87c-4563-d03e-c779eac0eaaa"
   },
   "outputs": [],
   "source": [
    "# all\n",
    "#Concatenate all text in the 'tweets' column\n",
    "all_text = ' '.join(df['tweets'])\n",
    "STOPWORDS.update(['ChatGPT', 'chatgpt', 'co', 'https', 'nhttps', 't','OpenAI'])\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width = 800, height = 400,\n",
    "                      background_color ='white',\n",
    "                      stopwords = STOPWORDS,\n",
    "                      min_font_size = 10).generate(all_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "E28N6bn9hlWu",
    "outputId": "6387be28-6795-4760-8e4f-060713cc2989"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Concatenate all text in the 'tweets' column\n",
    "all_text = ' '.join(df['tweets'])\n",
    "STOPWORDS.update(['ChatGPT', 'chatgpt', 'co', 'https', 'nhttps', 't','OpenAI'])\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(' '.join(df['tweets']))\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "Bc-_4ztQf53P",
    "outputId": "61dc1f3e-3f01-498c-9f44-41842bbb250b"
   },
   "outputs": [],
   "source": [
    "# good\n",
    "#Combine all the good reviews into a single string\n",
    "good_text = ' '.join(df[df['labels'] == 'good']['tweets'])\n",
    "STOPWORDS.update(['ChatGPT', 'chatgpt', 'co', 'https', 'nhttps', 't','OpenAI'])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=7).generate(good_text)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(12, 10), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "TONfhH6_iOXb",
    "outputId": "234b3903-45b2-4451-b826-a9ae361bfc0b"
   },
   "outputs": [],
   "source": [
    "#bad\n",
    "# Combine all the good reviews into a single string\n",
    "good_text = ' '.join(df[df['labels'] == 'bad']['tweets'])\n",
    "STOPWORDS.update(['ChatGPT', 'chatgpt', 'co', 'https', 'nhttps', 't','OpenAI'])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=7).generate(good_text)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(12, 10), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "ReQ1mksoiVwH",
    "outputId": "9371432e-ec58-4db4-ab27-ff8e02723e60"
   },
   "outputs": [],
   "source": [
    "# neutral\n",
    "#Combine all the good reviews into a single string\n",
    "good_text = ' '.join(df[df['labels'] == 'neutral']['tweets'])\n",
    "STOPWORDS.update(['ChatGPT', 'chatgpt', 'co', 'https', 'nhttps', 't','OpenAI'])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=7).generate(good_text)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(12, 10), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "camlsem4iuUS"
   },
   "source": [
    "Models selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyBtJqJ7oSoI"
   },
   "source": [
    "#1 Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3reX_l_itf_",
    "outputId": "6e454318-adf1-4b03-ebdf-e975389b79cf"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # downloads you a model\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "# print(stop)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "# return a list of tokens\n",
    "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
    "    # step 1: get sentences\n",
    "    sentences = sent_tokenize(doc)\n",
    "    # step 2: get tokens\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        # step 3 (optional): stemming\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        if need_sent:\n",
    "            tokens.append(words)\n",
    "        else:\n",
    "            tokens += words\n",
    "    return [w.lower() for w in tokens if w.lower() not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5-eOxDPoSoJ",
    "outputId": "2fad1feb-11f5-4bfd-e24f-9d6d2f17d353"
   },
   "outputs": [],
   "source": [
    "df.tweets.apply(pre_processing_by_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Zip'f Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtqR4X5txP8A"
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "import re\n",
    "\n",
    "corpus = ' '.join(list(df.tweets))\n",
    "\n",
    "new_corpus = re.sub(r'[^\\w\\s]', ' ', corpus)\n",
    "\n",
    "raw_tokens = new_corpus.lower().split()\n",
    "for token in raw_tokens:\n",
    "    freq[token] += 1\n",
    "order_tokens = sorted(list(freq.items()), key = lambda x : -x[1])\n",
    "print(order_tokens[:100])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = [freq for token, freq in order_tokens]\n",
    "\n",
    "plt.loglog(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCQiTkk7oSoJ"
   },
   "source": [
    "#2 Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8vN2l5LoSoJ",
    "outputId": "ae4ab419-5090-4365-e246-c8869e566d6c"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "DF = defaultdict(float)\n",
    "for doc in tqdm(df.tweets):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    for token in set(tokens):\n",
    "        DF[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3ySP46SoSoJ",
    "outputId": "c2df5d10-b5a6-447a-a278-6be440df49d8"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "IDF, vocab = dict(), dict()\n",
    "for token in DF:\n",
    "    if DF[token] < 50:\n",
    "        # this becomes an unk\n",
    "        pass\n",
    "    else:\n",
    "        vocab[token] = len(vocab)\n",
    "        IDF[token] = log(1 + len(df.tweets) / DF[token])\n",
    "print(len(DF), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkeO5xPRoSoJ",
    "outputId": "5e2f6eba-a6c6-4c32-c860-9f72c6b1f7ec"
   },
   "outputs": [],
   "source": [
    "IDF['<UNK>'] = 1\n",
    "vocab['<UNK>'] = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rj13lwm9oSoJ",
    "outputId": "66a7dab1-f66e-42b8-8752-142da2937606"
   },
   "outputs": [],
   "source": [
    "def tfidf_feature_extractor(doc, vocab, IDF):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in vocab:\n",
    "            tokens[i] = '<UNK>'\n",
    "    TF = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        TF[token] += 1\n",
    "    x = [0] * len(vocab)\n",
    "    for token in set(tokens):\n",
    "        tfidf = log(TF[token] + 1) * IDF[token]\n",
    "        token_id = vocab[token]\n",
    "#         print(token, TF[token], IDF[token])\n",
    "        x[token_id] = tfidf \n",
    "    return x\n",
    "\n",
    "x = tfidf_feature_extractor('tweets', vocab, IDF)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWqTzew8oSoK",
    "outputId": "ca96e860-841f-41f6-b410-81808d16bc8c"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for doc in tqdm(df.tweets):\n",
    "    X.append(tfidf_feature_extractor(doc, vocab, IDF))\n",
    "y = list(df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4Fx1HDzoSoK",
    "outputId": "a9b53884-84e6-4467-b4ca-b732065c4ee7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mB73JEpfoSoK",
    "outputId": "f2db4527-a695-44a4-a567-f07482f32382"
   },
   "outputs": [],
   "source": [
    "# train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train[:1000], y_train[:1000])\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvy_KN8loSoK"
   },
   "outputs": [],
   "source": [
    "# train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary-valued vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_exist_vocab(doc, vocab):\n",
    "    tokens = pre_processing_by_nltk(doc)  \n",
    "    for i, token in enumerate(tokens): \n",
    "         if token not in vocab:\n",
    "            tokens[i] = '<UNK>'\n",
    "    x = [0] * len(vocab)    \n",
    "    for token in set(tokens):  \n",
    "        token_id = vocab[token]\n",
    "        x[token_id] = 1\n",
    "    return x\n",
    "X = []\n",
    "for doc in tqdm(df.tweets):\n",
    "    X.append(if_exist_vocab(doc, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"AUROC:\",roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr'))\n",
    "print(\"Micro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"micro\"))\n",
    "print(\"Macro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(doc, vocab):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in vocab:\n",
    "            tokens[i] = '<UNK>'\n",
    "    x = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        token_id = vocab[token]\n",
    "        x[token_id] += 1   \n",
    "    return x\n",
    "X = []\n",
    "for doc in tqdm(df.tweets):\n",
    "    X.append(tf(doc, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"AUROC:\",roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr'))\n",
    "print(\"Micro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"micro\"))\n",
    "print(\"Macro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZZtmslv6C5z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=True,preprocessor=None,\n",
    "                        tokenizer=pre_processing_by_nltk,\n",
    "                        use_idf=True,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_opXJMj6GLr"
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df.tweets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G60wcM78oSoK"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"AUROC:\",roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr'))\n",
    "print(\"Micro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"micro\"))\n",
    "print(\"Macro F-1:\",f1_score(y_test,clf.predict(X_test) ,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "!pip install gensim\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(df.tweets.apply(pre_processing_by_nltk).values,min_count=3,vector_size=100,workers=3,window=5,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
