{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b868a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to build a vocabulary based on descending word frequencies \n",
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = vocabulary_inv[5:]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to learn word embeddings through Word2vec module\n",
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0279c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    # prepare translation table to translate punctuation to space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"tweets\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"tweets\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['The', 'chatgpt', 'openAI', 'https', 'nhttps', 'co'])\n",
    "\n",
    "def build_vocab(data):\n",
    "    word_counts = {}\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word not in stop_words and word not in string.punctuation:\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 1\n",
    "                else:\n",
    "                    word_counts[word] += 1\n",
    "\n",
    "    vocabulary_inv = [x[0] for x in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)]\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./\"\n",
    "\n",
    "df_train = pd.read_csv(data_path + \"file.csv\")\n",
    "df_test = pd.read_csv(data_path + \"file.csv\")\n",
    "\n",
    "df_train[\"tweets\"] = df_train[\"labels\"]+ \"\" + df_train[\"tweets\"]\n",
    "df_test[\"tweets\"] = df_test[\"labels\"]+ \"\"+ df_test[\"tweets\"]\n",
    "df_train = preprocess_df(df_train)\n",
    "df_test = preprocess_df(df_test)\n",
    "\n",
    "\n",
    "def preprocess_data(tagged_data):\n",
    "    special_characters = ['Chatgtp','chatgpt', 'https', 're', 'nhttps', 's',]\n",
    "    filtered_data = [[word for word in text if word not in special_characters] for text in tagged_data]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "\n",
    "# tokenization \n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"tweets\"])]\n",
    "# build vocabulary from tokenized data\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# use the above mapping to create input data\n",
    "inp_data = [[vocabulary[word] for word in text if (word!='chatgpt' and word!='Chatgtp'and word!='https'and word!='re' and word!='chatgpt' word!='nhttps'and word!='s')] for text in tagged_data ]\n",
    "# get embedding vector\n",
    "embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
    "\n",
    "\n",
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"tweets\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"tweets\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ff2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        if w!='The' and w!='chatgpt' and w!='openAI' and w!='https' and w!='nhttps':\n",
    "            vec += embedding_weights[vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "\n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            if w!='The' and w!='chatgpt' and w!='openAI' and w!='https' and w!='nhttps':\n",
    "                vec += embedding_weights[vocabulary[w]]\n",
    "                length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "param_grid = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "              'classifier__penalty': ['l1', 'l2'],\n",
    "              'classifier__C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 15, 20],\n",
    "              'classifier__solver': ['liblinear']}]\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 10, verbose = 1 , n_jobs = -1)\n",
    "#best = clf.fit(X_train, y_train)\n",
    "best = clf.fit(train_vec, df_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test data\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(test_vec)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(df_test[\"labels\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544ad56",
   "metadata": {},
   "source": [
    "SVC model code run without results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "pipe = Pipeline([('classifier', SVC())])\n",
    "param_grid = [{'classifier': [SVC()],\n",
    "              'classifier__kernel': ['linear', 'rbf'],\n",
    "              'classifier__C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 15, 20]}]\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, cv=10, verbose=1, n_jobs=-1)\n",
    "best = clf.fit(train_vec, df_train[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = clf.predict(test_vec)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(df_test[\"labels\"], y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
